# Ceph-deploy

![Untitled](Ceph-deploy%2016489c0c2bb34a938ec00a24d7b3f936/Untitled.png)

Prepare 4 machines in the same network as the topology above.

![Untitled](Ceph-deploy%2016489c0c2bb34a938ec00a24d7b3f936/Untitled%201.png)

**Note:** Run multiple OSD on the same host is not a bad idea, but we should not use multiple drive in a single storage device because it might reduce performance.

# Step 0 : Environment

Edit firewall to open these ports

- 22 for SSH
- 6789 for monitors
- 6800:7300 for OSDs, managers and metadata servers
- 8080 for dashboard
- 7480 for rados gateway

# Step 1: Install ceph-deploy and utility packets

ceph-deploy will be installed in Admin node only.

**Add Ceph Repository**

```cpp
wget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add -
echo deb https://download.ceph.com/debian-pacific/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
apt update
apt install ceph-deploy
```

- Configure the /etc/hosts file to resolve the resolution for ***node1, node2** and **node3** in **admin node only***
    
    **Improtant node :** The hostname of the node must be the same as the hostname specified in the hosts file of admin node.
    

Install NTP on **every nodes** for network timing synchronization

```cpp
sudo apt install ntp
```

# Step 2 : Deploy resources

```cpp
ceph-deploy new sld-ceph-1 sld-ceph-2 sld-ceph-3
```

Purpose:

- Display the deployment options of the ceph cluster.
- Create three configuration file in the current directory :
    - ceph.conf
    - ceph.mon.keyring
    - ceph-deploy-ceph.log
- It will then try to connect with the remote host via SSH. If the SSH is not configured by using key, then ceph-deploy will generate a key and copy it into the remote host, because the the admin node require passwordless access into all remote hosts.
- This command set the ***mon_initial_member*** in the ***ceph.conf*** file as the node that we specify earlier in the command.
    
    ![Untitled](Ceph-deploy%2016489c0c2bb34a938ec00a24d7b3f936/Untitled%202.png)
    

### Install ceph in all nodes

```bash
ceph-deploy install <node1> <node2> <node3>
```

This command will install ceph in all node. It will run the following command on each node

```bash
apt-get update
env DEBIAN_FRONTEND=noninteractive DEBIAN_PRIORITY=critical apt-get --assume-yes -q --no-install-recommends install ca-certificates apt-transport-https
apt-get install ceph ceph-osd ceph-mds ceph-mon radosgw
```

![Untitled](Ceph-deploy%2016489c0c2bb34a938ec00a24d7b3f936/Untitled%203.png)

### Initial monitor in the specified node

```cpp
ceph-deploy mon create-initial
```

The command will do the following

- Determine if provided hosthas the same hostname in remote
- Create kekyring file : /var/lib/ceph/tmp/ceph-sld-ceph-1.mon.keyring
- Running command: `ceph-mon --cluster ceph --mkfs -i sld-ceph-1 --keyring /var/lib/ceph/tmp/ceph-sld-ceph-1.mon.keyring --setuser 64045 --setgroup 64045`
- enable and start ceph-mon service
- show status of the whole cluster after finish create a mon host.
- Add each node into the **quorum**
- Gathering key

To summarize, the above command will deploy the monitor specified in the ceph.conf file in the admin node and gathering the key. The command is success if all the monitor node is up and reach the **quorum.** If the monitor node can not reach the quorum, it will retry 5 times, 5, 10, 15, 20, 25 second before the next trial.

### Push the ceph configuration file and the keyring to every nodes

```cpp
ceph-deploy admin <node1> <node2> <node3>
```

The above command will push ceph configuration file and keyring file into each nod, so **we can use the CLI without having to provide the ceph.client.admin.keyring each time execute a command. Chua hieu lam**

![Untitled](Ceph-deploy%2016489c0c2bb34a938ec00a24d7b3f936/Untitled%204.png)

To check the status of the cluster, ssh to a remote host and run command

```cpp
ceph status
```

![Untitled](Ceph-deploy%2016489c0c2bb34a938ec00a24d7b3f936/Untitled%205.png)

That means from an admin node with a keyring identified as admin, we can see the overview status of the ceph cluster.

For luminous+, build a manager daemon is required to monitor the clusterâ€™s state and modules/plugins. Use this command to deploy manager daemon in a node

```cpp
ceph-deploy mgr create sld-ceph-1
```

![Untitled](Ceph-deploy%2016489c0c2bb34a938ec00a24d7b3f936/Untitled%206.png)

In order to list all available disk on a targer machine, use command 

```cpp
ceph-deploy disk list sld-ceph-1
```

and the result

Attach one more disk in each node for ceph cluster, then run the following command to attach OSD daemon to that device.

![Untitled](Ceph-deploy%2016489c0c2bb34a938ec00a24d7b3f936/Untitled%207.png)

```cpp
ceph-deploy osd create --data /dev/vdb sld-ceph-1
```

Result of the command

```bash
root@sld-cephadm-adm:~# ceph-deploy osd create --data /dev/vdb sld-cephadm-1
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy osd create --data /dev/vdb sld-cephadm-1
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  subcommand                    : create
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf object at 0x7f0e07d65fa0>
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.cli][INFO  ]  func                          : <function osd at 0x7f0e07e06a60>
[ceph_deploy.cli][INFO  ]  data                          : /dev/vdb
[ceph_deploy.cli][INFO  ]  journal                       : None
[ceph_deploy.cli][INFO  ]  zap_disk                      : False
[ceph_deploy.cli][INFO  ]  fs_type                       : xfs
[ceph_deploy.cli][INFO  ]  dmcrypt                       : False
[ceph_deploy.cli][INFO  ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys
[ceph_deploy.cli][INFO  ]  filestore                     : None
[ceph_deploy.cli][INFO  ]  bluestore                     : None
[ceph_deploy.cli][INFO  ]  block_db                      : None
[ceph_deploy.cli][INFO  ]  block_wal                     : None
[ceph_deploy.cli][INFO  ]  host                          : sld-cephadm-1
[ceph_deploy.cli][INFO  ]  debug                         : False
[ceph_deploy.osd][DEBUG ] Creating OSD on cluster ceph with data device /dev/vdb
[sld-cephadm-1][DEBUG ] connected to host: sld-cephadm-1 
[ceph_deploy.osd][INFO  ] Distro info: ubuntu 20.04 focal
[ceph_deploy.osd][DEBUG ] Deploying osd to sld-cephadm-1
[sld-cephadm-1][WARNIN] osd keyring does not exist yet, creating one
[sld-cephadm-1][INFO  ] Running command: /usr/sbin/ceph-volume --cluster ceph lvm create --bluestore --data /dev/vdb
[sld-cephadm-1][WARNIN] Running command: /usr/bin/ceph-authtool --gen-print-key
[sld-cephadm-1][WARNIN] Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring -i - osd new 8716950d-f2f9-4020-b405-2a2d339fe407
[sld-cephadm-1][WARNIN] Running command: vgcreate --force --yes ceph-7aa4cf87-d872-4888-b860-3454225523ac /dev/vdb
[sld-cephadm-1][WARNIN]  stdout: Physical volume "/dev/vdb" successfully created.
[sld-cephadm-1][WARNIN]  stdout: Volume group "ceph-7aa4cf87-d872-4888-b860-3454225523ac" successfully created
[sld-cephadm-1][WARNIN] Running command: lvcreate --yes -l 7679 -n osd-block-8716950d-f2f9-4020-b405-2a2d339fe407 ceph-7aa4cf87-d872-4888-b860-3454225523ac
[sld-cephadm-1][WARNIN]  stdout: Logical volume "osd-block-8716950d-f2f9-4020-b405-2a2d339fe407" created.
[sld-cephadm-1][WARNIN] Running command: /usr/bin/ceph-authtool --gen-print-key
[sld-cephadm-1][WARNIN] Running command: /usr/bin/mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-0
[sld-cephadm-1][WARNIN] --> Executable selinuxenabled not in PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin
[sld-cephadm-1][WARNIN] Running command: /usr/bin/chown -h ceph:ceph /dev/ceph-7aa4cf87-d872-4888-b860-3454225523ac/osd-block-8716950d-f2f9-4020-b405-2a2d339fe407
[sld-cephadm-1][WARNIN] Running command: /usr/bin/chown -R ceph:ceph /dev/dm-0
[sld-cephadm-1][WARNIN] Running command: /usr/bin/ln -s /dev/ceph-7aa4cf87-d872-4888-b860-3454225523ac/osd-block-8716950d-f2f9-4020-b405-2a2d339fe407 /var/lib/ceph/osd/ceph-0/block
[sld-cephadm-1][WARNIN] Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /var/lib/ceph/osd/ceph-0/activate.monmap
[sld-cephadm-1][WARNIN]  stderr: 2024-04-03T23:18:42.632+0700 7f4acc864700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.bootstrap-osd.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory
[sld-cephadm-1][WARNIN] 2024-04-03T23:18:42.632+0700 7f4acc864700 -1 AuthRegistry(0x7f4ac8059430) no keyring found at /etc/ceph/ceph.client.bootstrap-osd.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,, disabling cephx
[sld-cephadm-1][WARNIN]  stderr: got monmap epoch 2
[sld-cephadm-1][WARNIN] Running command: /usr/bin/ceph-authtool /var/lib/ceph/osd/ceph-0/keyring --create-keyring --name osd.0 --add-key AQBhgQ1mlj6+OBAA5BIbY1WEHrPikYCUV4eeig==
[sld-cephadm-1][WARNIN]  stdout: creating /var/lib/ceph/osd/ceph-0/keyring
[sld-cephadm-1][WARNIN] added entity osd.0 auth(key=AQBhgQ1mlj6+OBAA5BIbY1WEHrPikYCUV4eeig==)
[sld-cephadm-1][WARNIN] Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-0/keyring
[sld-cephadm-1][WARNIN] Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-0/
[sld-cephadm-1][WARNIN] Running command: /usr/bin/ceph-osd --cluster ceph --osd-objectstore bluestore --mkfs -i 0 --monmap /var/lib/ceph/osd/ceph-0/activate.monmap --keyfile - --osd-data /var/lib/ceph/osd/ceph-0/ --osd-uuid 8716950d-f2f9-4020-b405-2a2d339fe407 --setuser ceph --setgroup ceph
[sld-cephadm-1][WARNIN]  stderr: 2024-04-03T23:18:42.804+0700 7fef3e6c3d80 -1 bluestore(/var/lib/ceph/osd/ceph-0/) _read_fsid unparsable uuid
[sld-cephadm-1][WARNIN]  stderr: 2024-04-03T23:18:42.812+0700 7fef3e6c3d80 -1 freelist read_size_meta_from_db missing size meta in DB
[sld-cephadm-1][WARNIN] --> ceph-volume lvm prepare successful for: /dev/vdb
[sld-cephadm-1][WARNIN] Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-0
[sld-cephadm-1][WARNIN] Running command: /usr/bin/ceph-bluestore-tool --cluster=ceph prime-osd-dir --dev /dev/ceph-7aa4cf87-d872-4888-b860-3454225523ac/osd-block-8716950d-f2f9-4020-b405-2a2d339fe407 --path /var/lib/ceph/osd/ceph-0 --no-mon-config
[sld-cephadm-1][WARNIN] Running command: /usr/bin/ln -snf /dev/ceph-7aa4cf87-d872-4888-b860-3454225523ac/osd-block-8716950d-f2f9-4020-b405-2a2d339fe407 /var/lib/ceph/osd/ceph-0/block
[sld-cephadm-1][WARNIN] Running command: /usr/bin/chown -h ceph:ceph /var/lib/ceph/osd/ceph-0/block
[sld-cephadm-1][WARNIN] Running command: /usr/bin/chown -R ceph:ceph /dev/dm-0
[sld-cephadm-1][WARNIN] Running command: /usr/bin/chown -R ceph:ceph /var/lib/ceph/osd/ceph-0
[sld-cephadm-1][WARNIN] Running command: /usr/bin/systemctl enable ceph-volume@lvm-0-8716950d-f2f9-4020-b405-2a2d339fe407
[sld-cephadm-1][WARNIN]  stderr: Created symlink /etc/systemd/system/multi-user.target.wants/ceph-volume@lvm-0-8716950d-f2f9-4020-b405-2a2d339fe407.service -> /lib/systemd/system/ceph-volume@.service.
[sld-cephadm-1][WARNIN] Running command: /usr/bin/systemctl enable --runtime ceph-osd@0
[sld-cephadm-1][WARNIN]  stderr: Created symlink /run/systemd/system/ceph-osd.target.wants/ceph-osd@0.service -> /lib/systemd/system/ceph-osd@.service.
[sld-cephadm-1][WARNIN] Running command: /usr/bin/systemctl start ceph-osd@0
[sld-cephadm-1][WARNIN] --> ceph-volume lvm activate successful for osd ID: 0
[sld-cephadm-1][WARNIN] --> ceph-volume lvm create successful for: /dev/vdb
[sld-cephadm-1][INFO  ] checking OSD status...
[sld-cephadm-1][INFO  ] Running command: /bin/ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host sld-cephadm-1 is now ready for osd use.

```

The above command will be used to create an osd that manage the device /dev/vdb in node sld-ceph-1.

The command will do the following action

**Create OSD**

```bash
sudo ceph-volume lvm create --data /dev/hdd1
```

**Prepare OSD**

```bash
sudo ceph-volume lvm prepare --data /dev/hdd1
```

**Activate OSD**

```bash
sudo ceph-volume lvm activate {ID} {FSID}
# Example
sudo ceph-volume lvm activate 0 a7f64266-0894-4f1e-a635-d0aeaca0e993
```