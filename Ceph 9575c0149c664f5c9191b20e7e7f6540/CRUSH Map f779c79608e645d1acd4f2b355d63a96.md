# CRUSH Map

# 1. Overview

CRUSH algorithm compute storage location to determine how to store and retreive data, allow Ceph clients to communicate directly with OSD rather than through a centralized server or broker → helps avoid SPOF, bottleneck and scalability limit

CRUSH use a map called CRUSH map to map data to OSD, distributing data across the whole cluster based on the configuration.

**Components of CRUSH Map**:

- A list of OSD
- hierarchy of buckets
- rule that govern how CRUSH replicate data within the cluster’s pools

Some factors relevant to the CRUSH hierarchy include chassis, racks, physical proximity, a shared power source, shared networking, and failure domains

# 2. CRUSH location

Location of an OSD refers to the location of it in the physical environment.

To specify the crush_location, we add crush_location field into the ceph.conf file

```yaml
crush_location = root=default row=a rack=a2 chassis=a2a host=a2a1
```

In case of deploying ceph in multiple datacenter, some location is not known, because of the different datacenter, we can omit it and create a custom location hook later by add a line into the ceph.conf file

```yaml
crush_location_hook = /path/to/customized-ceph-crush-location
```

This topic will be deep dived when handling a related task.

# 3. CRUSH Structure

A CRUSH map consists of

- A hierarchy that describes the physical topology of the cluster. The hierarchy of devices is grouped as **hosts, racks, rows, data centers,…**
- A set of rule that define data placement policy : determine how replicas is placed in terms of that hierarchy.

## 3.1 Devices

Devices are individual OSDs that store data

For Luminous release and later, OSD can have ***device class*** assigned (for example ssd, hdd, nvme), that will be used by rule to better distribute data to the device.

## 3.2 Types

- `osd` (or `device`)
- `host`
- `chassis`
- `rack`
- `row`
- `pdu`
- `pod`
- `room`
- `datacenter`
- `zone`
- `region`
- `root`

## 3.3 Bucket

A bucket is any of the internal node in the hierarchy (can be a host, a rack, a row,…)

## 3.4 Hierarchy

OSDs are always the leaves.

![Untitled](CRUSH%20Map%20f779c79608e645d1acd4f2b355d63a96/Untitled.png)

Each node has a ***weight,*** that specify the total amount of data that the bucket can be stored***.*** 

use command ceph-osd-tree to show the hierarchy of the cluster.

![Untitled](CRUSH%20Map%20f779c79608e645d1acd4f2b355d63a96/Untitled%201.png)

## 3.5 Rules

CRUSH rules define policy governing how data is distributed across the devices in the hierarchy.

Rule can define where to store data and which nodes are used to replicate data.

See all rule of that are definced for the cluster, use command 

```yaml
ceph osd crush rule ls
```

To view content of the rules, run the following command 

```yaml
ceph osd crush rule dump
```

![Untitled](CRUSH%20Map%20f779c79608e645d1acd4f2b355d63a96/Untitled%202.png)

## 3.5 Device class

By default, OSD automatically set their class at start up to hdd, ssd or nvme in accordance to their backed devices.

But, if you want to explicitly ser the device class of one or more OSD, run the following command

```yaml
ceph osd crush set-device-class <class> <osd-name> [...]
```

Unset class for an OSD. (Unset must be performed before re-assigning class for an OSD)

```yaml
ceph osd crush rm-device-class <osd-name> [...]
```

Create a placement rule that target a specific device class, run the command

```yaml
ceph osd crush rule create-replicated <rule-name> <root> <failure-domain> <class>
```

To create placemant rule targeting to a specific device class, run the command of the following form

```yaml
ceph osd pool set <pool-name> crush_rule <rule-name>
```

Device class are implemented by creating a one or more ***shadow*** CRUSH hierarchy. 

To show shadow items displayed, run th following command:

```yaml
ceph osd crush tree --show-shadow
```

## 3.6 Weight sets

Normal weight associated with each deivce in crush map are set based on the total amount of data can be stored. But this process is pseudo random, and weight set comes to rescue. It allows to perform numnericl optimization based in the soecifics of your cluster to achieve a balanced distribution.

Ceph support 2 types of weight sets:

- **Compat** weight set
- **per-pool** weight set

# 4. Modifying the crushmap

## 4.1 Add and removing an OSD

```yaml
ceph osd crush set {name} {weight} root={root} [{bucket-type}={bucket-name} ...]
# Example
ceph osd crush set osd.0 1.0 root=default datacenter=dc1 room=room1 row=foo rack=bar host=foo-bar-1
```

![Untitled](CRUSH%20Map%20f779c79608e645d1acd4f2b355d63a96/Untitled%203.png)

## 4.2 Reweight an OSD

```yaml
ceph osd reweight "osd.1" 2
```

![Untitled](CRUSH%20Map%20f779c79608e645d1acd4f2b355d63a96/Untitled%204.png)

## 4.3 Removing an OSD

```yaml
ceph osd crush remove "osd.2"
```

![Untitled](CRUSH%20Map%20f779c79608e645d1acd4f2b355d63a96/Untitled%205.png)

To andd that osd back to the crush map:

![Untitled](CRUSH%20Map%20f779c79608e645d1acd4f2b355d63a96/Untitled%206.png)

## 4.4 Moving a bucket

```yaml
ceph osd crush move {bucket-name} {bucket-type}={bucket-name}, [...]
# Example
ceph osd crush move foo root=default datacenter=dc2 room=room2
```

![Untitled](CRUSH%20Map%20f779c79608e645d1acd4f2b355d63a96/Untitled%207.png)

**Note** : Name of the bucket must be unique in the whole crush map. Incase you specify the destination **datacenter=dc2** and **room=room1**, even your bucket is in a different datacenter, but it sitll points to the room1 in the datacenter dc1.

## 4.5 Create a rule for a replicated pool

```yaml
ceph osd crush rule create-replicated {name} {root} {failure-domain-type} [{class}]
```

![Untitled](CRUSH%20Map%20f779c79608e645d1acd4f2b355d63a96/Untitled%208.png)

## 4.6 Create a rule for a erasure pool

List all exsting erasure-coding profile

```yaml
ceph osd erasure-code-profile ls
```

To view a specific existing profile, run a command of the following form:

```yaml
ceph osd erasure-code-profile get {profile-name}
```

Create a rule for erasure coding rule

```yaml
ceph osd crush rule create-erasure {name} {profile-name}
```